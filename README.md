# 桂林电子科技大学 2024 年软件开发实践课期末测试
## 题目一
### 题目描述

实验仿真分析所学的几种无约束优化算法（最速下降法，牛顿法，改进的牛顿法，共轭梯度法，拟牛顿法）的收敛性能。

**说明1**：主要包括（1）算法理论的简要描述；（2）仿真程序；（3）仿真结果简要分析；（4）算法性能对比分析。

**说明2**：选择恰当的仿真例子，可结合图表进行分析，写作格式自行安排，无统一要求。

### 算法理论简要概述
#### 最速下降法
##### 基本思想
- 最速下降法利用梯度下降法的思想，通过一阶泰勒展开式近似函数，并沿着负梯度方向（即函数下降最快的方向）移动。

$$f(x) \approx f(x_0) + \nabla f(x) (x-x_0)$$

$$d^{(k)} = -\nabla f(x)$$

$$x^{(k+1)} = x^{(k)} + \lambda d^{(k)}$$

- 步长 $\lambda_k$ 的确定通过一维搜索来实现，以确保在负梯度方向上函数值最小化。

$$\arg \min_{\lambda > 0} f \left( x^{(k)} + \lambda d^{(k)} \right)$$

##### 算法步骤
1. **初始化**：
   给定起始点 $x^0$ 和精度要求 $\delta > 0$。

2. **迭代过程**：
   - 计算当前点的梯度 $\nabla f(x^k)$，并确定下降方向：
     $$d^k = -\nabla f(x^k)$$
   - 如果梯度的范数满足停止条件 $\|d^k\| \leq \delta$，则算法停止，当前点为最优解。
   - 否则，进行**线搜索**，确定步长 $\lambda_k>0$，使得：

     $$f(x^k + \lambda_k d^k) = \min_{\lambda > 0} f(x^k + \lambda d^k)$$
   - 更新当前点：

     $$x^{k+1} = x^k + \lambda_k d^k$$

#### 牛顿法
##### 基本思想
牛顿法（经典牛顿法）利用二阶泰勒展开式对目标函数进行近似，因此会用到一阶导数（梯度）和二阶导数（黑塞矩阵）。牛顿法将近似得到二阶函数的极小值点作为原函数极小值点的近似，并不断重复这一近似的过程，直至得到满足精度要求的极小值点。牛顿法收敛速度非常快，不会随着问题维度的增大而大幅增加所需的迭代次数，且不需要通过线搜确定迭代更新的步长。

$$f(x) \approx f\left(x^{(k)}\right) + \nabla f(x^{(k)}) \left(x - x^{(k)}\right) + \frac{1}{2} \nabla^2 f(x^{(k)}) \left(x - x^{(k)}\right)^2$$ 

$$f'(x) = \nabla f(x^{(k)}) + \nabla^2 f(x^{(k)}) \left(x - x^{(k)}\right) = 0$$ 

$$x^{(k+1)} = x^{(k)} - \nabla^2 f(x^{(k)}) ^ {-1}\nabla f(x^{(k)})$$

##### 算法流程
1. **初始化**：
    - 选择初始点 $x^0 \in \mathbb{R}^n$。
    - 设定容差 $\delta > 0$ 和迭代次数 $k = 0$。

2. **迭代过程**：
    - 计算梯度 $g_k = \nabla f(x^k)$。
    - 若 $\| g_k \| \leq \delta$，停止迭代，当前点即为最优解 $x^* = x^k$。
    - 计算 Hessian 矩阵 $H_k = \nabla^2 f(x^k)$。
    - 更新迭代点: $$x^{k+1} = x^k - H_k^{-1} g_k$$
    - 迭代次数 $k$ 加 1 ，回第一步重新计算梯度。

#### 改进的牛顿法
##### 基本思想
改进牛顿法结合了牛顿法和线搜索的优点，通过线搜索确定最优步长 $\lambda_k$ ，以提高算法的稳定性，特别是在 Hessian 矩阵 $H_k$ 可能不正定时。改进牛顿法的核心思想是利用二阶泰勒展开确定搜索方向，并通过线搜索选择步长，确保每次迭代的目标函数值减小。

$$x^{(k+1)} = x^{(k)} - \lambda_k \nabla^2 f(x^{(k)}) ^ {-1}\nabla f(x^{(k)})$$

##### 算法流程
1. **初始化**：
    - 选择初始点 $x^0 \in \mathbb{R}^n$。
    - 设定容差 $\delta > 0$ 和迭代次数 $k = 0$。

2. **迭代过程**：
    - 计算梯度 $g_k = \nabla f(x^k)$。
    - 若 $\| g_k \| \leq \delta$，停止迭代，当前点即为最优解 $x^* = x^k$。
    - 计算 Hessian 矩阵 $H_k = \nabla^2 f(x^k)$。
    - 确定下降方向： $d^k = - H_k^{-1} g_k$
    - 线搜索找到步长$\lambda_k>0$： $$f(x^k + \lambda_k d^k) = \min_{\lambda > 0} f(x^k + \lambda d^k)$$
    - 更新迭代点 $$x^{k+1} = x^k + \lambda_k d_k$$
    - 迭代次数 $k$ 加 1 ，回第一步重新计算梯度。

#### F-R 共轭梯度法
##### 基本思想
共轭梯度法（Conjugate Gradient）是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点。

##### 算法流程
1. **初始化**：
    - 选择初始点 $x^0 \in \mathbb{R}^n$ 和精度 $\delta > 0$。
    - 计算初始搜索方向 $p_0 = -g_0 = -\nabla f(x^0)$，并设置迭代计数器 $k = 0$。
2. **迭代过程**：
    - 进行线搜索以找到最小值： $$\min_{\lambda \geq 0} f(x^k + \lambda p_k)$$
    - 更新迭代点： $$x^{k+1} = x^k + \lambda_k p_k$$
    - 计算新的梯度 $g_{k+1} = \nabla f(x^{k+1})$。
    - 如果梯度的范数 $\| g_{k+1} \| \leq \delta$，则停止迭代，并将当前点设为最优解 $x^* = x^{k+1}$；否则，继续下一步。
    - 更新搜索方向： $$p_{k+1} = -g_{k+1} + \alpha_k p_k, \alpha_k = \frac{g_{k+1}^T g_{k+1}}{g_k^T g_k}$$
    - 更新迭代计数器 $k = k + 1$ ，回第一步进行线搜索。

#### 拟牛顿法
##### 基本思想
牛顿法对目标的可导性更严格，要求二阶可导，有 Hesse 矩阵求逆的计算复杂的缺点。牛顿法中的 Hesse 矩阵 $H$ 在稠密时求逆计算量大，也有可能没有逆（ Hesse 矩阵非正定）。拟牛顿法提出，用不含二阶导数的矩阵 $B_k$ 替代牛顿法中的 $H_k^{-1}$ ，然后沿搜索方向 $-B_k g_k$ 做一维搜索。根据不同的 $B_k$ 构造方法有不同的拟牛顿法。


##### 算法流程
1. **初始化**：
    - 选择初始点 $x^0 \in R^n$ 和精度 $\delta > 0$。
    - 设置初始矩阵 $B_0 = I$，其中 $I$ 是单位矩阵，并设置迭代计数器 $k = 1$。
2. **迭代过程**：
    - 计算搜索方向 $p_k = -B_k g_k$，其中 $g_k = \nabla f(x^k)$。
    - 进行线性搜索以找到最小值： $$\min_{\lambda \geq 0} f(x^k + \lambda p_k)$$
    - 更新迭代点： $$x^{k+1} = x^k + \lambda_k p_k$$
    - 计算新的梯度 $g_{k+1} = \nabla f(x^{k+1})$。
    - 如果梯度的范数 $\| g_{k+1} \| \leq \delta$，则停止迭代，并将当前点设为最优解 $x^* = x^{k+1}$。
    - 计算差分向量： $$s_k = x^{k+1} - x^k, \quad y_k = g_{k+1} - g_k$$
    - 使用DFP公式或BFGS公式更新矩阵 $B_{k+1}$：
    
       $$B_{k+1} = B_k - \frac{B_k y_k y_k^T B_k}{y_k^T B_k y_k} + \frac{s_k s_k^T}{y_k^T s_k} \quad (\text{DFP})$$
   
       $$B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \frac{y_k y_k^T}{y_k^T s_k}  \quad (\text{BFGS})$$
   
    - 更新迭代计数器 $k = k + 1$，并返回到第一步继续迭代。

### Matlab 仿真程序
见附件test.m

### 算法结果和对比分析
#### 实验设置

测试函数： 

$$f(x_1,x_2) = (x_1 + x_2)^2 + (x_1 + 1)^2 + (x_2 + 3)^2$$

![alt text](/image/fun1.png)

最小点： $x^* = [\frac{1}{3}, - \frac{5}{3}]$，最小值： $f(x^*) = \frac{16}{3}.$
初值设为 $[-1,-1]$ ，精度设置为 $10^{-6}$ ，最大迭代次数为 10000 ，画出等高线及迭代点搜索过程图来进行观察。
#### 结果与分析

运行结果：

![alt text](/image/all_1.png)

牛顿法和改进的牛顿法在数值精度上表现最好，其次是F-R 共轭梯度法和拟牛顿法（BFGS），最后是拟牛顿法（DFP）和最速下降法。牛顿法的解析性质使其能快速且精确地找到最优解，特别是在目标函数是二次型或近似二次型的情况下。

各个算法的等高线图

![alt text](/image/all_7.png)

- **最速下降法**：经过 22 次迭代，算法终止。从图中可以观察到，最速下降法的路径是“锯齿状”的。算法在每一步都沿着梯度的方向前进，但由于目标函数的狭长谷底形状，导致频繁偏离目标点（局部最优路径），进而需要较多迭代才能收敛。
- **牛顿法**：由于牛顿法对于二次函数的一步收敛性，可见仅经过1次迭代，算法终止。从图中可以看到牛顿法几乎直接从起点跳到目标点，路径最短。这是由于牛顿法充分利用了二阶导数信息，具有快速的二次收敛性质。
- **改进的牛顿法**：在 2 次迭代内即可找到最优解，表现与牛顿法接近。其路径接近于牛顿法，但稍有偏离。这表明通过对 Hessian 矩阵的修正，改进牛顿法保持了快速收敛性，同时避免了某些病态情况。
- **共轭梯度法**：共轭梯度法具有“步收敛性”，即在每个方向都走到极致，不会走曾经走过的方向，一般 n 维空间只需要走 n 步即可找到极值点。图中仅需迭代次数仅需两次即收敛也证实了这一点。
- **拟牛顿法**：从结果可以看出，使用 BFGS 公式的拟牛顿法仅需两步即可收敛到最小值，而使用 DFP 公式的拟牛顿法则需要 16 步才能收敛。这一点说明了不同的更新公式对拟牛顿法的收敛速度是不同的，在这个例子中 BFGS 明显比 DFP 更优。BFGS 路径与共轭梯度法类似，BFGS 能通过逐步更新近似的 Hessian 矩阵实现快速收敛。DFP 路径明显比其他方法更曲折，需要多次迭代才能收敛，表明其更新规则在某些情况下的效率不如 BFGS。

等高线对比图

![alt text](/image/all_2.png)

最速下降法的路径与等高线的方向多次交叉，说明其搜索效率低下。
其他算法（牛顿法、改进牛顿法、共轭梯度法和 BFGS）沿着更接近目标的方向快速逼近最优解，说明其具有较好的搜索方向选择能力。

总结
牛顿法在收敛速度和精度上表现最佳，但计算代价较高，适合低维问题。
改进牛顿法、共轭梯度法和拟牛顿法（BFGS）是快速收敛的首选，适合中高维问题。
最速下降法和拟牛顿法（DFP）适用性较低，收敛速度较慢。

##### 进一步性能对比
将迭代次数固定为 10 次，对比精度和收敛速度。
运行结果如下：

![alt text](/image/all_4.png)
![alt text](/image/all_3.png)

从图中可以看出各算法在前几次迭代的函数值下降速度，以及最终函数值收敛的精度：

- **最速下降法**：
  - **表现**：最速下降法在前几次迭代中下降速度较快，但其下降曲线逐渐变平缓，显示出收敛速度缓慢的特点。
  - **数据验证**：10次迭代后最优值为 **5.3333358764648446**，精度低于其他方法。
  
- **牛顿法**：
  - **表现**：牛顿法在第1次迭代后就几乎达到最优值，后续基本保持稳定，验证了其二次收敛性质。
  - **数据验证**：最终函数值 **5.3333333333333339**，精度最高。

- **改进的牛顿法**：
  - **表现**：与牛顿法几乎一致，收敛速度极快，函数值在1~2次迭代后趋于平稳。
  - **数据验证**：最终函数值 **5.3333333333333339**，精度与牛顿法一致。

- **F-R 共轭梯度法**：
  - **表现**：共轭梯度法在2~3次迭代后迅速逼近最优值，其曲线稍低于牛顿法和改进牛顿法，表现出高效性。
  - **数据验证**：最终函数值 **5.3333333333333357**，与牛顿法精度接近。

- **拟牛顿法（BFGS）**：
  - **表现**：BFGS方法的收敛速度也非常快，曲线几乎与牛顿法和共轭梯度法重叠。
  - **数据验证**：最终函数值 **5.3333333333333321**，达到高精度。

- **拟牛顿法（DFP）**：
  - **表现**：DFP方法的函数值下降较慢，直到第5次迭代后才逐步逼近最优值，表现出明显低于BFGS的效率。
  - **数据验证**：最终函数值 **5.3333333509672025**，精度略逊于其他方法。

**总结**

- 牛顿法、改进牛顿法、BFGS和F-R共轭梯度法在收敛过程中表现出较好的稳定性，函数值的曲线平滑且迅速下降。
- 最速下降法曲线较平缓，表明其迭代步长容易受目标函数形状的影响，收敛过程不稳定。
- DFP方法的下降曲线较为迟缓，可能由于其更新规则对目标函数的曲率信息利用不充分。

##### 更换测试函数进行对比
采用 2 阶 Rastrigin 函数进行稳定性测试，函数表达式和三维图像如下：

$$f(x_1,x_2) = 20 + x_1^2 + x_2^2 - 10 \cos(2 \pi x_1) - 10 \cos(2 \pi x_2)$$

![alt text](/image/fun2.png)

可以看到该函数有很多局部极小值点，最小值点为 $[0,0]$。初始设置和上一个函数一样，运行结果如下：

![alt text](/image/all_6.png)
![alt text](/image/all_5.png)

从结果可以看出，牛顿法和改进的牛顿法都陷入了局部最小值点，而其他方法都可以成功地收敛到最小值点，说明牛顿法稳定性较差，而其他方法比较稳定。分析可能是初始点的选取不佳造成的，因为牛顿法对初始点的选取很敏感。当然这个结果只能算是特例，可以更换其他初始点进行更多测试来验证。
